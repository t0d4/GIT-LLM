training:
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  num_train_epochs: 8
  dataloader_num_workers: 0
  fp16: true
  optim: "adamw_torch"
  learning_rate: 5.0e-5
  logging_steps: 100
  evaluation_strategy: "steps"
  save_strategy: "steps"
  eval_steps: 200
  save_steps: 1000
  deepspeed: configs/ds_config_zero1.json
  output_dir: /data/toda/GNN-OPT/output-opt-2.7b-multi-inst
  report_to: "wandb"

settings:
  model_name: facebook/opt-2.7b
  max_length: 768
  keys_finetune:
    - graph_projection

use_lora: false
lora:
  r: 8
  lora_alpha: 32
  target_modules:
    - q_proj
    - v_proj
  lora_dropout: 0.01
  bias: none
  task_type: CAUSAL_LM

